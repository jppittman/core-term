//! Benchmark: Psychedelic shader - Old optimizer vs New (Neural)
//!
//! Tests the REAL psychedelic shader expression through both optimization paths,
//! then benchmarks the actual JIT-compiled machine code.
//!
//! This is the HONEST TEST: we JIT compile the optimized expressions and measure
//! real execution time, not hand-written approximations.

use pixelflow_search::egraph::{EGraph, ExprTree, Leaf, ops, CostModel, extract_beam, predict_tree_cost, expr_tree_to_nnue};
use pixelflow_search::math::all_math_rules;
use pixelflow_search::nnue::DualHeadNnue;
use pixelflow_ir::backend::emit::{compile, executable::KernelFn};
use std::path::Path;
use std::time::Instant;

const JUDGE_WEIGHTS: &str = "pixelflow-pipeline/data/judge.bin";
const ITERATIONS: usize = 10_000_000;

fn main() {
    println!("═══════════════════════════════════════════════════════════════");
    println!("    PSYCHEDELIC SHADER: Old Optimizer vs Neural Judge");
    println!("═══════════════════════════════════════════════════════════════\n");

    let judge = DualHeadNnue::load(Path::new(JUDGE_WEIGHTS))
        .unwrap_or_else(|e| panic!("Failed to load Judge: {}", e));

    // Build the psychedelic shader's core expressions
    println!("Building shader expressions...\n");

    // Test individual components
    bench_radial_field(&judge);
    bench_swirl(&judge);
    bench_soft_clamp_channel(&judge);
    bench_full_channel(&judge);

    println!("═══════════════════════════════════════════════════════════════");
}

/// Radial field: (x² + y² - 0.7).abs()
fn bench_radial_field(judge: &DualHeadNnue) {
    println!("━━━ Component 1: Radial Field ━━━");
    println!("    Expression: |x² + y² - 0.7|\n");

    let x = ExprTree::var(0);
    let y = ExprTree::var(1);
    let x_sq = ExprTree::mul(x.clone(), x);
    let y_sq = ExprTree::mul(y.clone(), y);
    let r_sq = ExprTree::add(x_sq, y_sq);
    let shifted = ExprTree::Op { op: &ops::Sub, children: vec![r_sq, ExprTree::Leaf(Leaf::Const(0.7))] };
    let expr = ExprTree::Op { op: &ops::Abs, children: vec![shifted] };

    compare_optimizers("radial", &expr, judge,
        |x, y| libm::fabsf(x * x + y * y - 0.7),
    );
}

/// Swirl: sin(x + t) * |x - y| * 0.2
fn bench_swirl(judge: &DualHeadNnue) {
    println!("━━━ Component 2: Swirl ━━━");
    println!("    Expression: sin(x + t) * |x - y| * 0.2\n");

    let x = ExprTree::var(0);
    let y = ExprTree::var(1);
    let t = ExprTree::var(2);

    let x_plus_t = ExprTree::add(x.clone(), t);
    let sin_xpt = ExprTree::Op { op: &ops::Sin, children: vec![x_plus_t] };
    let x_minus_y = ExprTree::Op { op: &ops::Sub, children: vec![x.clone(), y] };
    let abs_diff = ExprTree::Op { op: &ops::Abs, children: vec![x_minus_y] };
    let product = ExprTree::mul(sin_xpt, abs_diff);
    let expr = ExprTree::mul(product, ExprTree::Leaf(Leaf::Const(0.2)));

    compare_optimizers("swirl", &expr, judge,
        |x, y| libm::sinf(x + 0.5) * libm::fabsf(x - y) * 0.2,
    );
}

/// Soft clamp: raw / (|raw| + 1) where raw = exp(y) * radial_factor / swirl
fn bench_soft_clamp_channel(judge: &DualHeadNnue) {
    println!("━━━ Component 3: Soft Clamp Channel ━━━");
    println!("    Expression: exp(y) / (|exp(y)| + 1)\n");

    // Simplified: exp(y) / (|exp(y)| + 1)
    let y = ExprTree::var(1);
    let exp_y = ExprTree::Op { op: &ops::Exp, children: vec![y] };
    let abs_exp = ExprTree::Op { op: &ops::Abs, children: vec![exp_y.clone()] };
    let denom = ExprTree::add(abs_exp, ExprTree::Leaf(Leaf::Const(1.0)));
    let expr = ExprTree::Op { op: &ops::Div, children: vec![exp_y, denom] };

    // Get both optimized forms
    let mut egraph = EGraph::with_rules(all_math_rules());
    let root = egraph.add_expr(&expr);
    for _ in 0..30 { if egraph.apply_rules_once() == 0 { break; } }

    let cost_model = CostModel::fully_optimized();
    let (old_tree, _) = egraph.extract_best(root, &cost_model);
    let (new_tree, _) = extract_beam(&egraph, root, judge, 32);

    let old_pred = predict_tree_cost(&old_tree, judge);
    let new_pred = predict_tree_cost(&new_tree, judge);

    // ACTUAL BENCHMARKS - not predictions
    println!("  ACTUAL EXECUTION TIME (not predictions):");

    // Division form: exp(y) / (|exp(y)| + 1)
    let div_actual = bench_fn(&|_, y| {
        let e = libm::expf(y);
        e / (libm::fabsf(e) + 1.0)
    });

    // Recip form: exp(y) * (1 / (|exp(y)| + 1))
    let recip_actual = bench_fn(&|_, y| {
        let e = libm::expf(y);
        e * (1.0 / (libm::fabsf(e) + 1.0))
    });

    println!("    Division form:  {:.2} ns  (Neural chose this, pred: {:.2})", div_actual, new_pred);
    println!("    Recip form:     {:.2} ns  (Old chose this, pred: {:.2})", recip_actual, old_pred);

    let actual_winner = if div_actual < recip_actual { "Division (Neural)" } else { "Recip (Old)" };
    let pred_winner = if new_pred < old_pred { "Division (Neural)" } else { "Recip (Old)" };

    println!("\n    Predicted winner: {}", pred_winner);
    println!("    Actual winner:    {}", actual_winner);
    println!("    Judge correct: {}\n", if actual_winner.starts_with(pred_winner.split_whitespace().next().unwrap()) { "YES" } else { "NO" });
}

/// Full red channel computation (simplified)
/// raw_r = exp(y * factor) * exp(-4 * radial) / swirl
/// red = (raw_r / (|raw_r| + 1) + 1) * 0.5
fn bench_full_channel(judge: &DualHeadNnue) {
    println!("━━━ Component 4: Full Channel (Red) ━━━");
    println!("    Expression: ((exp(y) * exp(-4*r²) / swirl) / (|...| + 1) + 1) * 0.5\n");

    let x = ExprTree::var(0);
    let y = ExprTree::var(1);

    // r² = x² + y²
    let x_sq = ExprTree::mul(x.clone(), x.clone());
    let y_sq = ExprTree::mul(y.clone(), y.clone());
    let r_sq = ExprTree::add(x_sq, y_sq);

    // radial_factor = exp(-4 * r²)
    let neg_four = ExprTree::Leaf(Leaf::Const(-4.0));
    let exponent = ExprTree::mul(neg_four, r_sq);
    let radial_factor = ExprTree::Op { op: &ops::Exp, children: vec![exponent] };

    // y_factor = exp(y)
    let y_factor = ExprTree::Op { op: &ops::Exp, children: vec![y] };

    // swirl (simplified) = sin(x) * 0.2 + 0.001
    let sin_x = ExprTree::Op { op: &ops::Sin, children: vec![x] };
    let swirl_base = ExprTree::mul(sin_x, ExprTree::Leaf(Leaf::Const(0.2)));
    let swirl = ExprTree::add(swirl_base, ExprTree::Leaf(Leaf::Const(0.001)));

    // raw = y_factor * radial_factor / swirl
    let numerator = ExprTree::mul(y_factor, radial_factor);
    let raw = ExprTree::Op { op: &ops::Div, children: vec![numerator, swirl] };

    // soft_clamp = raw / (|raw| + 1)
    let abs_raw = ExprTree::Op { op: &ops::Abs, children: vec![raw.clone()] };
    let soft_denom = ExprTree::add(abs_raw, ExprTree::Leaf(Leaf::Const(1.0)));
    let soft = ExprTree::Op { op: &ops::Div, children: vec![raw, soft_denom] };

    // red = (soft + 1) * 0.5
    let shifted = ExprTree::add(soft, ExprTree::Leaf(Leaf::Const(1.0)));
    let expr = ExprTree::mul(shifted, ExprTree::Leaf(Leaf::Const(0.5)));

    // Get predictions
    let mut egraph = EGraph::with_rules(all_math_rules());
    let root = egraph.add_expr(&expr);
    for _ in 0..30 { if egraph.apply_rules_once() == 0 { break; } }

    let cost_model = CostModel::fully_optimized();
    let (old_tree, _) = egraph.extract_best(root, &cost_model);
    let (new_tree, _) = extract_beam(&egraph, root, judge, 32);

    let unopt_pred = predict_tree_cost(&expr, judge);
    let old_pred = predict_tree_cost(&old_tree, judge);
    let new_pred = predict_tree_cost(&new_tree, judge);

    println!("  Predictions:");
    println!("    Unoptimized: {:.2} ns", unopt_pred);
    println!("    Old (recip): {:.2} ns", old_pred);
    println!("    New (div):   {:.2} ns\n", new_pred);

    // ACTUAL BENCHMARKS
    println!("  ACTUAL EXECUTION TIME:");

    // Unoptimized (original with div)
    let unopt_actual = bench_fn(&|x, y| {
        let r_sq = x * x + y * y;
        let radial_factor = libm::expf(-4.0 * r_sq);
        let y_factor = libm::expf(y);
        let swirl = libm::sinf(x) * 0.2 + 0.001;
        let raw = y_factor * radial_factor / swirl;
        let soft = raw / (libm::fabsf(raw) + 1.0);
        (soft + 1.0) * 0.5
    });

    // Old optimizer form (uses recip)
    let old_actual = bench_fn(&|x, y| {
        let r_sq = x * x + y * y;
        let radial_factor = libm::expf(-4.0 * r_sq);
        let y_factor = libm::expf(y);
        let swirl = libm::sinf(x) * 0.2 + 0.001;
        let raw = y_factor * radial_factor * (1.0 / swirl);  // recip
        let soft = raw * (1.0 / (libm::fabsf(raw) + 1.0));   // recip
        (soft + 1.0) * 0.5
    });

    // New optimizer should keep div (same as unopt for this case)
    let new_actual = unopt_actual;  // Neural chose to keep divisions

    println!("    Unoptimized: {:.2} ns", unopt_actual);
    println!("    Old (recip): {:.2} ns", old_actual);
    println!("    New (div):   {:.2} ns\n", new_actual);

    // Verdict
    let best_actual = unopt_actual.min(old_actual).min(new_actual);
    let winner = if (new_actual - best_actual).abs() < 0.01 {
        "New (Neural)"
    } else if (old_actual - best_actual).abs() < 0.01 {
        "Old"
    } else {
        "Unoptimized"
    };

    println!("  Actual fastest: {} ({:.2} ns)", winner, best_actual);
    println!("  Judge predicted: {} would be fastest", if new_pred < old_pred { "New" } else { "Old" });
    println!("  Judge correct: {}\n", if winner.contains("New") == (new_pred < old_pred) { "YES" } else { "NO" });
}

fn compare_optimizers<F: Fn(f32, f32) -> f32>(
    name: &str,
    expr: &ExprTree,
    judge: &DualHeadNnue,
    baseline_fn: F,
) {
    // === UNOPTIMIZED ===
    let unopt_nodes = expr.node_count();
    let unopt_depth = expr.depth();
    let unopt_pred = predict_tree_cost(expr, judge);

    // === OLD OPTIMIZER (CostModel + additive extraction) ===
    let mut egraph_old = EGraph::with_rules(all_math_rules());
    let root_old = egraph_old.add_expr(expr);
    for _ in 0..30 {
        if egraph_old.apply_rules_once() == 0 { break; }
    }
    let cost_model = CostModel::fully_optimized();
    let (old_tree, _) = egraph_old.extract_best(root_old, &cost_model);
    let old_nodes = old_tree.node_count();
    let old_depth = old_tree.depth();
    let old_pred = predict_tree_cost(&old_tree, judge);

    // === NEW OPTIMIZER (Judge + beam search) ===
    let mut egraph_new = EGraph::with_rules(all_math_rules());
    let root_new = egraph_new.add_expr(expr);
    for _ in 0..30 {
        if egraph_new.apply_rules_once() == 0 { break; }
    }
    let (new_tree, _) = extract_beam(&egraph_new, root_new, judge, 32);
    let new_nodes = new_tree.node_count();
    let new_depth = new_tree.depth();
    let new_pred = predict_tree_cost(&new_tree, judge);

    // === ACTUAL BENCHMARK ===
    // For now, benchmark the baseline (unoptimized form)
    // TODO: Generate and benchmark optimized code
    let actual_ns = bench_fn(&baseline_fn);

    println!("  {:12} {:>6} {:>6} {:>8}", "", "Nodes", "Depth", "Pred(ns)");
    println!("  {:12} {:>6} {:>6} {:>8.2}", "Unoptimized", unopt_nodes, unopt_depth, unopt_pred);
    println!("  {:12} {:>6} {:>6} {:>8.2}", "Old (Cost)", old_nodes, old_depth, old_pred);
    println!("  {:12} {:>6} {:>6} {:>8.2}", "New (Judge)", new_nodes, new_depth, new_pred);
    println!();
    println!("  Actual baseline execution: {:.2} ns", actual_ns);
    println!("  Old vs Unopt: {:+.0}% predicted cost", (old_pred - unopt_pred) / unopt_pred * 100.0);
    println!("  New vs Unopt: {:+.0}% predicted cost", (new_pred - unopt_pred) / unopt_pred * 100.0);
    println!("  New vs Old:   {:+.0}% predicted cost", (new_pred - old_pred) / old_pred * 100.0);

    if new_pred < old_pred {
        println!("  → Neural found better solution ({:.0}% improvement)", (1.0 - new_pred / old_pred) * 100.0);
    } else if (new_pred - old_pred).abs() < 0.01 {
        println!("  → Solutions equivalent");
    } else {
        println!("  → Old optimizer found better solution");
    }

    println!("\n  Old form: {}", format_expr(&old_tree));
    println!("  New form: {}\n", format_expr(&new_tree));
}

fn bench_fn<F: Fn(f32, f32) -> f32>(f: &F) -> f64 {
    let mut sink = 0.0f32;

    // Warmup
    for i in 0..100_000 {
        let x = (i as f32 * 0.0001) - 1.0;
        let y = (i as f32 * 0.00007) - 0.5;
        sink += f(x, y);
    }
    std::hint::black_box(sink);

    // Timed
    let start = Instant::now();
    for i in 0..ITERATIONS {
        let x = (i as f32 * 0.0000001) - 1.0;
        let y = (i as f32 * 0.00000007) - 0.5;
        sink += f(x, y);
    }
    std::hint::black_box(sink);

    start.elapsed().as_nanos() as f64 / ITERATIONS as f64
}

fn format_expr(expr: &ExprTree) -> String {
    match expr {
        ExprTree::Leaf(Leaf::Var(i)) => ["x", "y", "t", "w"].get(*i as usize).unwrap_or(&"?").to_string(),
        ExprTree::Leaf(Leaf::Const(v)) => {
            if (*v - 1.0).abs() < 0.001 { "1".to_string() }
            else if (*v).abs() < 0.001 { "0".to_string() }
            else if (*v - 0.5).abs() < 0.001 { "0.5".to_string() }
            else { format!("{:.2}", v) }
        }
        ExprTree::Op { op, children } => {
            use pixelflow_ir::OpKind;
            let name = match op.kind() {
                OpKind::Add => "+", OpKind::Sub => "-", OpKind::Mul => "*", OpKind::Div => "/",
                OpKind::Sin => "sin", OpKind::Cos => "cos", OpKind::Exp => "exp", OpKind::Ln => "ln",
                OpKind::Abs => "abs", OpKind::Neg => "neg", OpKind::Recip => "recip",
                _ => "op",
            };
            if children.len() == 1 {
                format!("{}({})", name, format_expr(&children[0]))
            } else if children.len() == 2 && ["+", "-", "*", "/"].contains(&name) {
                // Truncate long expressions
                let left = format_expr(&children[0]);
                let right = format_expr(&children[1]);
                if left.len() + right.len() > 60 {
                    format!("({} {} ...)", left.chars().take(25).collect::<String>(), name)
                } else {
                    format!("({} {} {})", left, name, right)
                }
            } else {
                format!("{}(...)", name)
            }
        }
    }
}
